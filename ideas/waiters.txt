Goal:
	A general-purpose mechanism for one or more tasks to be put to sleep
	until another task (or OS activity) triggers a wake of associated sleeping tasks.
This low-level syncronization primitive allows all manner of condition waiting,
without needing to resort to busy looping.
Examples:
	Waiting on long-lived (multiple time slices) exclusive locks to become available
	Waiting for a full queue (including OS-managed queues like graphics tile queue) to be non-full
	Waiting for an empty queue to be non-empty
And therefore solves the following current problems that we're currently busy-looping on:
	Waiting for a full graphics queue to have room to write new tiles
	Waiting for a button to be pressed

Design:
	Needs to be able to hold multiple task ids to wake
	However most common case will probably only be one task id
	Given this is something we need to wait for a while for, taking a fair chunk of CPU time should be ok.
	Option A: single task or malloc'd array
		3 forms:
			0, junk
			1, TASK_ID
			pointer to array
		Note the array address will never be in 0000-01ff (that's ROM, array is RAM) so 0,1 as magic values is safe.
		A leading 0 indicates no waiters, a leading 1 indicates a single waiter, specified as next byte, and anything else
		indicates it is a pointer to a ff-terminated array of task ids.
		Note allocated memory would need to always be MAX_TASKS long to avoid needing to realloc if we grow.
		Note we'd need to loop over all waiters everywhere (error-prone?) in order to remove dead task ids.
	Option B: Central list
		We observe that a task will only ever be waiting on at most one waiter.
		We keep a global list mapping task ids to waiter "IDs", either literally as a list of (task, waiter id)
		or by making current waiter an element of task.
		This makes invalidating dead tasks easy, but makes management harder since waiter ids now need to be allocated,
		and just in general makes them a special case.
		We could partially get around this by making the ids (bank, addr) though you need to be careful about WRAM0 vs WRAMX vs SRAMX.
	Option C: A bit of both
		We want waking to be fast if there are few or no tasks to wake (motivating example: joypad interrupt), so needing to loop through
		every task is a poor option. At the same time, we want a good way of un-waiting a task on death. So, we do:
			- Waiter struct has form:
				count, lowest task id (junk if count == 0)
			- Task struct contains a 3 byte value (waiter bank, waiter addr)
				Note: we might be able to pack this down. Bank is at most 4 bits (for SRAM), addr is 15 bits since first bit can't be 0 (that's ROM)
				but since it also wont be VRAM, echo RAM, OAM, etc. it actually only has $4000 + 127 (HRAM) legal values.
				Best to keep it simple though.
				At the very least there needs to be a sentinel value.
				For now, let's define it thus:
					bank = 0 - No bank or bank 0
					bank = $ff - No waiter
					else - bank is bank that contains waiter, check addr for which bank (WRAM, SRAM) to set, addr contains addr
						alternate: SRAM and WRAM banks can be encoded together with a nibble each. We should adopt this consistently. A 0 in either means N/A.
		Then all operations are fast, simple and consistent:
			Wait:
				Inc count
				Check if new task id < current saved task id (unless count was 0). If so, replace it.
				Go to task struct for new task id, point it at this waiter.
			Wake:
				If count is 0, do nothing
				Starting at saved task id, and proceeding until count tasks have been found (or end of list),
					find tasks in task list which point at this waiter
				Enqueue those tasks, and clear their waiter pointers
				Set count to 0.
			Cleanup:
				Nothing required beyond just resetting the task struct.

			Note that if cleanup occurs for a task that is the saved task id for a waiter, the task id is not guarenteed to be
			pointing at that waiter, but IS still guarenteed to be <= all tasks that do.

		Only downside is a bigger task list struct, which can be solved by a) having the task pointers list as a seperate array,
		or b) revamping the task list so that task ids are indexes rather than offsets (eg. task after task 0 is task 1 instead of task 4 or task TASK_SIZE)
